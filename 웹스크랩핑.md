# 웹스크랩핑

> 웹 상의 데이터를 가져오는 방법



### 1. 웹의 작동원리와 데이터 요청

#### 1.1 웹의 작동원리



![image-20200718105502469](C:\Users\note\AppData\Roaming\Typora\typora-user-images\image-20200718105502469.png)

* ex) 검색엔진에서 검색을 하는 행위가 서버에 요청(request)을 보내는 행위라 할 수 있음. 

#### 1.2 웹 스크랩핑

> 요청(request) -> 웹 페이지 분석 및 추출(parsing)

* requests

  > 서버에 데이터를 요청할 때 사용하는 라이브러리

``` python
import requests

url='' #url 객체 생성
req = requests.get(url) #데이터 요청 메소드 get에 url넘김
print(req.status_code) #요청 결과 확인(200번대이면 성공적으로 데이터 반환한 것)
```



* selenium

  > 동적인 방식으로 서버에 데이터 요청하는 라이브러리
  >
  > 데이터가 html 페이지 상 존재하지 않고, javascript를 통해 전달됨
  >
  > cf) requests의 응답 코드에는 데이터가 없음. 실제 사용자가 웹브라우저를 실행시키고 요청   하는 방식이 필요함. 

```python
from selenium import webdriver

driver = webdriverzChrome() # 크롬 실행파일이 있는 장소 지정
driver.quit()
```



#### 1.3 웹 페이지의 구조

* HTML

  > 다수의 웹 페이지가 작성된 언어 

  ![image-20200718111108143](C:\Users\note\AppData\Roaming\Typora\typora-user-images\image-20200718111108143.png)
  * 브라우저는 태그명을 인식해서 페이지의 구조를 표시함

* XML

  > 다양한 종류의 데이터를 주고 받기 위해 만들어진 언어

* JSON

  > 데이터 교환을 목적으로 만들어진 언어. 보다 직관적으로 데이터 해석 가능.
  >
  > 파이썬의 사전형 자료와 유사한 구조를 가지고 있음
  >
  > {"key" : "value"}

  

  - json 패키지를 이용하여 보기 좋은 형태로 출력 가능

```python
json_data = json.loads(req.text)
for data in json_data['items']:
    print("기사제목 : {0}, 기사 링크 : {1}". format(data['title',data['link']]))
```





#### 1.4 웹 페이지 분석

* BeautifulSoup

  > HTML, XML 페이지에서 데이터를 파악하고 추출하는데 특화된 기능을 가진 라이브러리

```python
from bs4 import beautifulsoup
import requests

url='' 
req = requests.get(url)
html = req.content

soup = beautifulsoup(html, 'html.parser') # html파일과 parser를 전달
```

  *  BeautifulSoup 주요 명령어

![image-20200718112945679](C:\Users\note\AppData\Roaming\Typora\typora-user-images\image-20200718112945679.png)

#### 1.5 API와 사용법

* API

  > 사용자가 서버로부터 데이터를 보다 편리하게 가져올 수 있도록 만들어주는 인터페이스

  * 웹 페이지의 변동에 영향을 받지 않음
  * 웹 스크랩핑 방식과 달리 웹 페이지 구문 분석을 요구하지 않음
  * 웹 페이지 각각의 api를 숙지해야 함

  







