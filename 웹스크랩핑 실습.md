# 웹스크랩핑 실습

### 1. find 함수

> find / find_all 차이점 정리하기 **

* daum 에서 실시간 검색어 추출

```python
import requests
from bs4 import BeautifulSoup

url = 'https://www.daum.net/'

req = requests.get(url)

data = BeautifulSoup(req.content,'html.parser')

select = data.find_all(class_="list_favorsch")

for i in select:
    print(i.text)
```



* visitbusan 홈페이지에서 부산시 명소 데이터 추출

```python
import requests
from bs4 import BeautifulSoup
import csv  # csv저장 라이브러리

# csv 파일에 저장하기
f = open('busan2.csv', 'w', newline='', encoding='utf-8-sig')
csv_writer = csv.writer(f)

url = 'https://www.visitbusan.net/index.do?menuCd=DOM_000000201001000000'

req = requests.get(url)

data = BeautifulSoup(req.content, 'html.parser')

q = data.find_all("p", class_="tit")

# 명소 이름 print
#for i in q:
    #print(i.text)

# 명소 이름 csv에 저장
for i,a in enumerate(q):
    csv_writer.writerow([i,q[i].text])

f.close()

```

  - cf ) `int 를 range로 사용하려면` 

    ```python
    for i in range(1,len(q)+1)
    ```

- cf2 ) `error`

  구글 크롬 개발자도구에서 셀렉터 카피에서 나오는 child 선택자인 nth-child 를 지원하지 않는다.

  ***tr:nth ->nth-of-type 으로 바꿔준다.***

### 2. select 함수

> select는 리스트 형태이고, select_one은 아니다.

* select

  * daum 에서 실시간 검색어 추출

  ```python
  import requests
  from bs4 import BeautifulSoup
  
  url = 'https://www.daum.net/'
  
  req = requests.get(url).text
  
  data = BeautifulSoup(req,'html.parser')
  
  #select는 리스트로 받아옴
  
  select3 = data.select('#wrapSearch > div.slide_favorsch > ul:nth-of-type(1)')
  
  for i in select3:
      print(i.text)
  
  #select_one은 리스트가 아님
  
  select2 = data.select_one('#wrapSearch > div.slide_favorsch')
  print(select2.text)
  
  ```

  * visitbusan 홈페이지에서 부산시 명소 데이터 추출

```python
url = 'https://www.visitbusan.net/index.do?menuCd=DOM_000000201001000000'

req = requests.get(url)

data = BeautifulSoup(req.content, 'html.parser')

for i in range(1,len(q)+1):
    select3 = data.select(f'#content > div > div.travel_list > div.trvList > div > div:nth-of-type({str(i)}) > div.info > p.tit > a')
    b = [k for k in select3] #리스트 형태에서 각각의 텍스트 추출하는 방법
    print(b)
```



### EX) 부산시 공식 관광 포털'비짓 부산'홈페이지에서 부산시 명소 데이터추출

```python
from bs4 import BeautifulSoup #웹크롤링을 위한 뷰리풀숩 패키지 임포트
import requests #웹크롤링을 위한 리퀘스트 패키지 임포트
import csv #csv 파일 관련 라이브러리 호출을 위한 패키지 임포트

#csv 파일에 저장하기
f = open('busan.csv','w',newline='',encoding='utf-8-sig')
csv_writer = csv.writer(f)

#총 7페이지의 내용 가져오기

for i in range(1,7):

# receive html
    res = requests.get("https://www.visitbusan.net/index.do?uc_seqs=&ucMtmUcSeq=&ucMtmItemUcSeq=&file_name=&gugun_nm=&cate2_nm=&ucc1_seq=15&cate1_nm=&ucdpp_seqs=&uct_seqs=&ucum_seqs=&ucl_seq=7&ucl_use_yn=Y&exclude_uc_seq=&place=&title=&subtitl=&hash_tag=&menuCd=DOM_000000201001000000&list_type=TYPE_BIG_CARD&order_type=NEW&listCntPerPage2=16&ucum_seq=&ub_seq=&distance=0.0&cate2_month=&favoriteThis=N&myFavoriteUserId=&sel_visit_place=N&user_id=&search_keyword=&num_room=&ulg_seq=&ucc2_seq=&ucg_seq=&ucogl_seq=&main_img_ucmf_seq=&main_title=&charger_positn=&charger_nm=&charger_tel=&tripadvisor_id=&lat=&lng=&bundle_cntnts_yn=&use_yn=Y&sort_num=&page_no="+str(i))

# html page parsing #페이지 내용 중 명소 이름, 명소 위치, 명소와 관련된 설명만 추출함.

    soup = BeautifulSoup(res.content,'html.parser')
    travel_title = soup.find_all("p", class_="tit") #명소 이름
    travel_place = soup.find_all(class_="loc") #명소 위치
    travel_content = soup.find_all(class_="Lcon") #명소와 관련된 설명


    for i,a in enumerate(travel_title):
        csv_writer.writerow([i, travel_title[i].get_text(),travel_place[i].get_text(),travel_content[i].get_text()])

f.close()
```



